version: '3.8'

services:
  # PostgreSQL Database
  postgres:
    image: postgres:15
    container_name: postgres_eventsparis
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "5433:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./sql/init_dw.sql:/docker-entrypoint-initdb.d/init_dw.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - eventsparis_network

  # Airflow Init
  airflow_init:
    build: .
    container_name: airflow_init
    environment:
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:5432/${POSTGRES_DB}
      AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
      AIRFLOW_UID: ${AIRFLOW_UID}
      # Env vars for Python scripts
      MONGO_HOST: ${MONGO_HOST}
      MONGO_PORT: ${MONGO_PORT}
      MONGO_USER: ${MONGO_USER}
      MONGO_PASSWORD: ${MONGO_PASSWORD}
      MONGO_DB: ${MONGO_DB}
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PORT: ${POSTGRES_PORT}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    depends_on:
      postgres:
        condition: service_healthy
    command: >
      bash -c "
        airflow db migrate &&
        (airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin || echo 'User already created')
      "
    networks:
      - eventsparis_network

  # Apache Airflow Webserver
  airflow_webserver:
    build: .
    container_name: airflow_webserver
    environment:
      AIRFLOW__CORE__DAGS_FILES: /opt/airflow/dags
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:5432/${POSTGRES_DB}
      AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'False'
      AIRFLOW_UID: ${AIRFLOW_UID}
      PYTHONPATH: /opt/airflow/src
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_SECRET_KEY}
      AIRFLOW__WEBSERVER__WORKER_TIMEOUT: '120'
      # Env vars for Python scripts
      MONGO_HOST: ${MONGO_HOST}
      MONGO_PORT: ${MONGO_PORT}
      MONGO_USER: ${MONGO_USER}
      MONGO_PASSWORD: ${MONGO_PASSWORD}
      MONGO_DB: ${MONGO_DB}
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PORT: ${POSTGRES_PORT}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "8080:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./src:/opt/airflow/src
    depends_on:
      postgres:
        condition: service_healthy
      airflow_init:
        condition: service_completed_successfully
    command: airflow webserver
    networks:
      - eventsparis_network

  # Apache Airflow Scheduler
  airflow_scheduler:
    build: .
    container_name: airflow_scheduler
    environment:
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:5432/${POSTGRES_DB}
      AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'False'
      AIRFLOW_UID: ${AIRFLOW_UID}
      PYTHONPATH: /opt/airflow/src
      # Env vars for Python scripts
      MONGO_HOST: ${MONGO_HOST}
      MONGO_PORT: ${MONGO_PORT}
      MONGO_USER: ${MONGO_USER}
      MONGO_PASSWORD: ${MONGO_PASSWORD}
      MONGO_DB: ${MONGO_DB}
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PORT: ${POSTGRES_PORT}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./src:/opt/airflow/src
    depends_on:
      postgres:
        condition: service_healthy
      airflow_init:
        condition: service_completed_successfully
    command: airflow scheduler
    networks:
      - eventsparis_network

  # Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus_eventsparis
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    networks:
      - eventsparis_network

  mongodb_exporter:
    image: percona/mongodb_exporter:0.40.0
    container_name: mongodb_exporter_eventsparis
    environment:
      MONGODB_URI: mongodb://${MONGO_USER}:${MONGO_PASSWORD}@${MONGO_HOST}:${MONGO_PORT}/?authSource=admin
    command:
      - "--collect-all"
    ports:
      - "9216:9216"
    depends_on:
      - mongodb
    networks:
      - eventsparis_network

  postgres_exporter:
    image: prometheuscommunity/postgres-exporter
    container_name: postgres_exporter_eventsparis
    ports:
      - "9187:9187"
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:5432/${POSTGRES_DB}?sslmode=disable"
    depends_on:
      - postgres
    networks:
      - eventsparis_network

  # Grafana
  grafana:
    image: grafana/grafana:latest
    container_name: grafana_eventsparis
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER}
      GF_INSTALL_PLUGINS: 'grafana-clock-panel,grafana-simple-json-datasource,grafana-worldmap-panel'
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning/datasources/datasource.yml:/etc/grafana/provisioning/datasources/datasource.yml
      - ./grafana/provisioning/dashboards/dashboard.yml:/etc/grafana/provisioning/dashboards/dashboard.yml
      - ./grafana/provisioning/dashboards/mongo_dashboard.json:/var/lib/grafana/dashboards/mongo_dashboard.json
      - ./grafana/provisioning/dashboards/postgres_dashboard.json:/var/lib/grafana/dashboards/postgres_dashboard.json
    depends_on:
      - prometheus
    networks:
      - eventsparis_network

  # Python Dashboard (Streamlit)
  dashboard:
    build: .
    container_name: dashboard_eventsparis
    ports:
      - "8501:8501"
    environment:
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - ./src:/opt/airflow/src
    entrypoint: []
    command: bash -c "export PATH=$PATH:/home/airflow/.local/bin && streamlit run /opt/airflow/src/dashboard.py --server.address 0.0.0.0"
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - eventsparis_network

  # Trigger DAGs on startup
  airflow_trigger:
    build: .
    container_name: airflow_trigger
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:5432/${POSTGRES_DB}
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW_UID: ${AIRFLOW_UID}
    depends_on:
      airflow_scheduler:
        condition: service_started
      airflow_init:
        condition: service_completed_successfully
    restart: on-failure
    command: >
      bash -c "
      echo 'Waiting for DAGs to be parsed...' &&
      sleep 30 &&
      airflow dags trigger extract_paris_events || echo 'Failed to trigger events DAG' &&
      airflow dags trigger speedtest_monitor || echo 'Failed to trigger speedtest DAG'
      "
    networks:
      - eventsparis_network

  # Initial Data Loading Service (ETL)
  etl_init:
    build: .
    container_name: etl_init
    environment:
      AIRFLOW_UID: ${AIRFLOW_UID}
      PYTHONPATH: /opt/airflow/src
      # Env vars for Python scripts
      MONGO_HOST: ${MONGO_HOST}
      MONGO_PORT: ${MONGO_PORT}
      MONGO_USER: ${MONGO_USER}
      MONGO_PASSWORD: ${MONGO_PASSWORD}
      MONGO_DB: ${MONGO_DB}
    volumes:
      - ./src:/opt/airflow/src:z
    depends_on:
      - mongodb
    restart: on-failure
    command: >
      bash -c "pip install pymongo requests && python /opt/airflow/src/extract.py"
    networks:
      - eventsparis_network

  mongodb:
    image: mongo:latest
    container_name: mongodb_eventsparis
    ports:
      - "${MONGO_PORT}:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_USER}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_PASSWORD}
    volumes:
      - mongo_data:/data/db
    networks:
      - eventsparis_network


volumes:
  postgres_data:
  prometheus_data:
  grafana_data:
  mongo_data:

networks:
  eventsparis_network:
    driver: bridge

